

import numpy as np
import pandas as pd
import requests
import keras
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import keras.backend as K

tf.config.run_functions_eagerly(True)

url= 'https://github.com/TuseAsrav/Physics-Informed-Neural-Networks-and-Hyper-parameter-Optimization-for-Dynamic-Process-Systems/blob/main/semibatch2.xlsx?raw=true'
myfile = requests.get(url)

df=pd.read_excel(myfile.content)
dff=df.drop(['time','dk','deltaCC'],axis=1)
data=np.array(dff)

n=data.shape[0]
d=data.shape[1]

normalized_data=np.zeros((n,d))

for i in range(n):
  for j in range(d):
    normalized_data[i,j]=2*((data[i,j]-np.min(data[:,j]))/(np.max(data[:,j])-np.min(data[:,j])))-1

new_set= pd.DataFrame(normalized_data, columns = dff.columns)

x=new_set.iloc[:,0:3]
y=new_set.iloc[:,2:3]

training_set, test_set, training_output, test_output=train_test_split(x,y, test_size=0.7, shuffle=False)

def backnormA(x): 
  backnormCA=(x+1)/2*(np.max(data[:,0]-np.min(data[:,0])))+np.min(data[:,0])

  return backnormCA

def backnormB(x): 
  backnormCB=(x+1)/2*(np.max(data[:,1]-np.min(data[:,1])))+np.min(data[:,1])

  return backnormCB

def backnormC(x): 
  backnormCC=(x+1)/2*(np.max(data[:,2]-np.min(data[:,2])))+np.min(data[:,2])

  return backnormCC

trainingEQ=df.iloc[5:150,-3].values
deltaCCtraining=df.iloc[5:150,-1].values

def custom_loss_function(data, y_pred):
    
   y_true=data[:,None,0]
   CA=data[:,None,1]
   CB=data[:,None,2]
   EQ=data[:,None,3]
   deltaCC=data[:,None,4]
   squared_difference = tf.square((y_true - y_pred))
   mse = tf.reduce_mean(squared_difference, axis=-1)
   y_prednonscaled=backnormC(y_pred.numpy())
   Ca=backnormA(CA.numpy())
   Cb=backnormB(CB.numpy())
   phyloss3=tf.square(deltaCC-0.2*Ca*Cb+EQ*y_prednonscaled)
   phyloss=tf.reduce_sum(phyloss3, axis=-1)

   return mse+phyloss

def create_data_sequence(X, y, time_steps=1):

    input_sequence, output= [], []
    for i in range(len(X) - time_steps):
        sequence = X.iloc[i:(i + time_steps)].values
        input_sequence.append(sequence)        
        output.append(y.iloc[i + time_steps])
        
    return np.array(input_sequence), np.array(output)

time_steps = 5

training_set_sequence, training_set_output = create_data_sequence(training_set, training_set.CC, time_steps)
test_set_sequence, test_set_output = create_data_sequence(test_set, test_set.CC, time_steps)

model = keras.Sequential()
model.add(keras.layers.SimpleRNN(units=25,activation='tanh',return_sequences=True,input_shape=(training_set_sequence.shape[1], training_set_sequence.shape[2])))
model.add(keras.layers.SimpleRNN(units=25,activation='tanh',return_sequences=False))
model.add(keras.layers.Dense(units=1))
model.compile(loss=custom_loss_function, optimizer='Adam')
model.summary()

es=tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)
    
history = model.fit(
        training_set_sequence,
        np.hstack((training_set_output.reshape((145,1)),training_set.iloc[5:,0].values.reshape((145,1)),training_set.iloc[5:,1].values.reshape((145,1)),trainingEQ.reshape((145,1)),deltaCCtraining.reshape((145,1)))), 
        epochs=400, 
        batch_size=25, 
        shuffle=False,
        callbacks=[es]
    )

model2 = keras.Sequential()
model2.add(keras.layers.LSTM(units=25,activation='tanh',return_sequences=True,input_shape=(training_set_sequence.shape[1], training_set_sequence.shape[2])))
model2.add(keras.layers.LSTM(units=25,activation='tanh',return_sequences=False))
model2.add(keras.layers.Dense(units=1))
model2.compile(loss=custom_loss_function, optimizer='Adam')
model2.summary()

history2 = model2.fit(
        training_set_sequence,
        np.hstack((training_set_output.reshape((145,1)),training_set.iloc[5:,0].values.reshape((145,1)),training_set.iloc[5:,1].values.reshape((145,1)),trainingEQ.reshape((145,1)),deltaCCtraining.reshape((145,1)))), 
        epochs=400, 
        batch_size=25, 
        shuffle=False,
        callbacks=[es]
    )

model3 = keras.Sequential()
model3.add(keras.layers.GRU(units=25,activation='tanh',return_sequences=True,input_shape=(training_set_sequence.shape[1], training_set_sequence.shape[2])))
model3.add(keras.layers.GRU(units=25,activation='tanh',return_sequences=False))
model3.add(keras.layers.Dense(units=1))
model3.compile(loss=custom_loss_function, optimizer='Adam')
model3.summary()

history3 = model3.fit(
        training_set_sequence,
        np.hstack((training_set_output.reshape((145,1)),training_set.iloc[5:,0].values.reshape((145,1)),training_set.iloc[5:,1].values.reshape((145,1)),trainingEQ.reshape((145,1)),deltaCCtraining.reshape((145,1)))), 
        epochs=400, 
        batch_size=25, 
        shuffle=False,
        callbacks=[es]
    )

test_set_predictions = model.predict(test_set_sequence)

model_predictions = backnormC(test_set_predictions)
actual_testset_values = backnormC(test_set_output)

mse=metrics.mean_squared_error(actual_testset_values, model_predictions)
print('SimpleRNN:Test MSE: ', mse)

train_set_predictions = model.predict(training_set_sequence)

model_trainpredictions= backnormC(train_set_predictions)
actual_trainingset_values = backnormC(training_set_output)

mse=metrics.mean_squared_error(actual_trainingset_values, model_trainpredictions)
print('SimpleRNN:Train MSE: ', mse)

test_set_predictions2 = model2.predict(test_set_sequence)

model2_predictions = backnormC(test_set_predictions2)

mse=metrics.mean_squared_error(actual_testset_values, model2_predictions)
print('LSTM:Test MSE: ', mse)

train_set_predictions2 = model3.predict(training_set_sequence)

model2_trainpredictions= backnormC(train_set_predictions2)

mse=metrics.mean_squared_error(actual_trainingset_values, model2_trainpredictions)
print('LSTM:Train MSE: ', mse)

test_set_predictions3 = model3.predict(test_set_sequence)

model3_predictions = backnormC(test_set_predictions3)

mse=metrics.mean_squared_error(actual_testset_values, model3_predictions)
print('GRU:Test MSE: ', mse)

train_set_predictions3 = model3.predict(training_set_sequence)

model3_trainpredictions= backnormC(train_set_predictions3)

mse=metrics.mean_squared_error(actual_trainingset_values, model3_trainpredictions)
print('GRU:Train MSE: ', mse)


fig,ax = plt.subplots()
x=df['time']
plt.plot(x[150:-5,],model_predictions, label='SimpleRNN:Predicted Test CC')
plt.plot(x[150:-5,],model2_predictions, label='LSTM:Predicted Test CC')
plt.plot(x[150:-5,],model3_predictions, label='GRU:Predicted Test CC')
plt.plot(x[150:-5,],actual_testset_values, label='Actual Test CC')
ax.set_xlabel('Minutes')
ax.set_ylabel('CC')
plt.legend();
plt.show()

fig,ax = plt.subplots()
x=df['time']
plt.plot(x[5:150,],model_trainpredictions, label='SimpleRNN:Predicted Train CC')
plt.plot(x[5:150,],model2_trainpredictions, label='LSTM:Predicted Train CC')
plt.plot(x[5:150,],model3_trainpredictions, label='GRU:Predicted Train CC')
plt.plot(x[5:150,],actual_trainingset_values, label='Actual Train CC')
ax.set_xlabel('Minutes')
ax.set_ylabel('CC')
plt.legend();
plt.show()
